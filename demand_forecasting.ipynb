{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpOvM0MM9i3I"
      },
      "source": [
        "# Profit-Driven Demand Forecasting\n",
        "> Data Mining Cup 2020 solution with graident boosted trees \n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- author: Nikita Kozodoi\n",
        "- categories: [python, time series, demand forecasting, competitions]\n",
        "- image: images/posts/demand.png\n",
        "- cover: images/covers/demand.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9X1wMkm9i3M"
      },
      "source": [
        "*Last update: 21.03.2021*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PVkdyHg9i3N"
      },
      "source": [
        "# 1. Overview\n",
        "\n",
        "Demand forecasting is an important task that helps to optimize inventory planning. Optimized stocks reduce retailer's costs and increase customer satisfaction due to faster delivery time. \n",
        "\n",
        "The 2020 edition of the [Data Mining Cup](https://www.data-mining-cup.com) was devoted to profit-driven demand prediction for a set of items using past purchase data. Together with [Elizaveta Zinovyeva](https://www.linkedin.com/in/elizaveta-zinovyeva-4155a184/), we represented the [Humboldt University of Berlin](https://www.hu-berlin.de/en?set_language=en) and finished in the top-15 of the leaderboard.\n",
        "\n",
        "This blog post provides a detailed walkthrough covering the crucial steps of our solution:\n",
        "- data preparation and feature engineering\n",
        "- aggregation of transactional data into the daily format\n",
        "- implementation of custom profit loss functions\n",
        "- two-stage demand forecasting with LightGBM\n",
        "- hyper-parameter tuning with `hyperopt`\n",
        "\n",
        "Feel free to jump directly to the sections interesting to you! The code with our solution is available [on Github](https://github.com/kozodoi/DMC_2020)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f9KFjYC9i3O"
      },
      "source": [
        "# 2. Data preparation\n",
        "\n",
        "## Data overview\n",
        "\n",
        "The competition data includes three files: \n",
        "- `items.csv`: item-specific characteristics such as brand, manufacturer, etc\n",
        "- `orders.csv`: purchase transactions over the 6-month period\n",
        "- `infos.csv`: prices and promotions in the unlabeled test set\n",
        "\n",
        "Let's have a look at the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icRB9RHB9i3O",
        "outputId": "cd417f09-0e8d-4ba7-8108-52f28ab34188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10463, 3)\n",
            "(10463, 8)\n",
            "(2181955, 5)\n"
          ]
        }
      ],
      "source": [
        "# packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# data import\n",
        "infos  = pd.read_csv('../data/raw/infos.csv',  sep = '|')\n",
        "items  = pd.read_csv('../data/raw/items.csv',  sep = '|')\n",
        "orders = pd.read_csv('../data/raw/orders.csv', sep = '|')\n",
        "print(infos.shape)\n",
        "print(items.shape)\n",
        "print(orders.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HDVvdmt9i3Q",
        "outputId": "d31b6300-b195-406f-a117-dfdfa0b1ca0a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>itemID</th>\n",
              "      <th>simulationPrice</th>\n",
              "      <th>promotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.43</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>9.15</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>14.04</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   itemID  simulationPrice promotion\n",
              "0       1             3.43       NaN\n",
              "1       2             9.15       NaN\n",
              "2       3            14.04       NaN\n",
              "3       4            14.10       NaN\n",
              "4       5             7.48       NaN"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "infos.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i3U9rqZ9i3R",
        "outputId": "53eccdee-fa82-477d-f3df-802651f80bfd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>itemID</th>\n",
              "      <th>brand</th>\n",
              "      <th>manufacturer</th>\n",
              "      <th>customerRating</th>\n",
              "      <th>category1</th>\n",
              "      <th>category2</th>\n",
              "      <th>category3</th>\n",
              "      <th>recommendedRetailPrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>16.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5.00</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>15.89</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   itemID  brand  manufacturer  customerRating  category1  category2  \\\n",
              "0       1      0             1            4.38          1          1   \n",
              "1       2      0             2            3.00          1          2   \n",
              "2       3      0             3            5.00          1          3   \n",
              "3       4      0             2            4.44          1          2   \n",
              "4       5      0             2            2.33          1          1   \n",
              "\n",
              "   category3  recommendedRetailPrice  \n",
              "0          1                    8.84  \n",
              "1          1                   16.92  \n",
              "2          1                   15.89  \n",
              "3          1                   40.17  \n",
              "4          1                   17.04  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "items.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OSBYSB39i3R",
        "outputId": "f9d9172e-a5e5-438d-bede-7b72301dc7f2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>transactID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>order</th>\n",
              "      <th>salesPrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2018-01-01 00:01:56</td>\n",
              "      <td>2278968</td>\n",
              "      <td>450</td>\n",
              "      <td>1</td>\n",
              "      <td>17.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2018-01-01 00:01:56</td>\n",
              "      <td>2278968</td>\n",
              "      <td>83</td>\n",
              "      <td>1</td>\n",
              "      <td>5.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2018-01-01 00:07:11</td>\n",
              "      <td>2255797</td>\n",
              "      <td>7851</td>\n",
              "      <td>2</td>\n",
              "      <td>20.47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  time  transactID  itemID  order  salesPrice\n",
              "0  2018-01-01 00:01:56     2278968     450      1       17.42\n",
              "1  2018-01-01 00:01:56     2278968      83      1        5.19\n",
              "2  2018-01-01 00:07:11     2255797    7851      2       20.47\n",
              "3  2018-01-01 00:09:24     2278968     450      1       17.42\n",
              "4  2018-01-01 00:09:24     2278968      83      1        5.19"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "orders.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ2VmB_I9i3S"
      },
      "source": [
        "For each of the 10,463 items, we need to predict the total number of orders in the 14-day period following the last day in `orders`.\n",
        "\n",
        "## Preprocessing\n",
        "\n",
        "Let's prepare the data! First, we merge item-level data in `items` and `infos`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RBhlPYd9i3S",
        "outputId": "638fb8d1-3814-4d67-b4a4-805bdab5fd83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10463, 3)\n",
            "(10463, 8)\n",
            "(10463, 10)\n"
          ]
        }
      ],
      "source": [
        "print(infos.shape)\n",
        "print(items.shape)\n",
        "items = pd.merge(infos, items, on = 'itemID', how = 'left')\n",
        "print(items.shape)\n",
        "del infos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V5sR9XR9i3T"
      },
      "source": [
        "Next, we check and convert feature types to the appropriate format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg_iqUE49i3T",
        "outputId": "3530b75b-6e05-4d45-eb5c-d489f9ff0f24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "itemID                      int64\n",
            "simulationPrice           float64\n",
            "promotion                  object\n",
            "brand                       int64\n",
            "manufacturer                int64\n",
            "customerRating            float64\n",
            "category1                   int64\n",
            "category2                   int64\n",
            "category3                   int64\n",
            "recommendedRetailPrice    float64\n",
            "dtype: object\n",
            "--------------------------------------------------\n",
            "time           object\n",
            "transactID      int64\n",
            "itemID          int64\n",
            "order           int64\n",
            "salesPrice    float64\n",
            "dtype: object\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#collapse-hide\n",
        "\n",
        "print('-' * 50)\n",
        "print(items.dtypes)\n",
        "print('-' * 50)\n",
        "print(orders.dtypes)\n",
        "print('-' * 50)\n",
        "\n",
        "# items\n",
        "for var in ['itemID', 'brand', 'manufacturer', 'category1', 'category2', 'category3']:\n",
        "    items[var] = items[var].astype('str').astype('object') \n",
        "    \n",
        "# orders\n",
        "for var in ['transactID', 'itemID']:\n",
        "    orders[var] = orders[var].astype('str').astype('object') \n",
        "    \n",
        "# dates\n",
        "orders['time'] = pd.to_datetime(orders['time'].astype('str'), infer_datetime_format = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFwsAxvV9i3T"
      },
      "source": [
        "Finally, we unfold the `promotion` feature containing a sequence of coma-separated dates. We use `split_nested_features()` from `dptools` to split a string column into separate features.\n",
        "\n",
        "`dptools` is a package developed by me to simplify common data preprocessing and feature engineering tasks. Below, you will see more examples on using `dptools` for other applications. You can read more about the package [here](https://github.com/kozodoi/dptools)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uu6HmzIe9i3U",
        "outputId": "62bc1ba4-77b8-4218-ffbf-80adc63ccedb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 3 split-based features.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>itemID</th>\n",
              "      <th>simulationPrice</th>\n",
              "      <th>brand</th>\n",
              "      <th>manufacturer</th>\n",
              "      <th>customerRating</th>\n",
              "      <th>category1</th>\n",
              "      <th>category2</th>\n",
              "      <th>category3</th>\n",
              "      <th>recommendedRetailPrice</th>\n",
              "      <th>promotion_0</th>\n",
              "      <th>promotion_1</th>\n",
              "      <th>promotion_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.43</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8.84</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>9.15</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>16.92</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>14.04</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>5.00</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>15.89</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  itemID  simulationPrice brand manufacturer  customerRating category1  \\\n",
              "0      1             3.43   NaN            1            4.38         1   \n",
              "1      2             9.15   NaN            2            3.00         1   \n",
              "2      3            14.04   NaN            3            5.00         1   \n",
              "3      4            14.10   NaN            2            4.44         1   \n",
              "4      5             7.48   NaN            2            2.33         1   \n",
              "\n",
              "  category2 category3  recommendedRetailPrice promotion_0 promotion_1  \\\n",
              "0         1         1                    8.84         NaN         NaN   \n",
              "1         2         1                   16.92         NaN         NaN   \n",
              "2         3         1                   15.89         NaN         NaN   \n",
              "3         2         1                   40.17         NaN         NaN   \n",
              "4         1         1                   17.04         NaN         NaN   \n",
              "\n",
              "  promotion_2  \n",
              "0         NaN  \n",
              "1         NaN  \n",
              "2         NaN  \n",
              "3         NaN  \n",
              "4         NaN  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#collapse-show\n",
        "\n",
        "# import packages\n",
        "!pip install dptools\n",
        "from dptools import *\n",
        "\n",
        "# split promotion feature\n",
        "items = split_nested_features(items, split_vars = 'promotion', sep = ',')\n",
        "print(items.head(3))\n",
        "\n",
        "# convert dates\n",
        "promotion_vars = items.filter(like = 'promotion_').columns\n",
        "for var in promotion_vars:\n",
        "    items[var] = pd.to_datetime(items[var], infer_datetime_format = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDj6oL2M9i3U"
      },
      "source": [
        "We now export the data as csv. I use `save_csv_version()` to automatically add a version number to the file name to prevent overwriting the data after making changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDHfBQyD9i3U",
        "outputId": "271ff515-bf5c-4ecc-f6bd-6d110b69c4da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved as ../data/prepared/orders_v2.csv\n",
            "Saved as ../data/prepared/items_v2.csv\n"
          ]
        }
      ],
      "source": [
        "save_csv_version('../data/prepared/orders.csv', orders, index = False, compression = 'gzip')\n",
        "save_csv_version('../data/prepared/items.csv',  items,  index = False, compression = 'gzip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dZUUiUS9i3V"
      },
      "source": [
        "# 3. Aggregation and feature engineering\n",
        "\n",
        "## Data aggregation\n",
        "\n",
        "Let's work with `orders`, which provides a list of transactions with timestamps. \n",
        "\n",
        "We need to aggregate this data for future modeling. Since the task is a 14-day demand forecasting, a simple way would be to aggregate transactions on a two-week basis. However, this could lead to losing some more granular information. We aggregate transactions by day:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcHhrjd09i3V",
        "outputId": "9d451490-7b53-455f-d839-8b3cb25a9cbe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>itemID</th>\n",
              "      <th>day_of_year</th>\n",
              "      <th>order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>307</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   itemID  day_of_year  order\n",
              "0       1           23      1\n",
              "1       1           25      1\n",
              "2       1           29    307\n",
              "3       1           30      3\n",
              "4       1           31      1"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#collapse-show\n",
        "orders['day_of_year'] = orders['time'].dt.dayofyear\n",
        "orders_price = orders.groupby(['itemID', 'day_of_year'])['salesPrice'].agg('mean').reset_index()\n",
        "orders = orders.groupby(['itemID', 'day_of_year'])['order'].agg('sum').reset_index()\n",
        "orders.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nJU5p199i3V"
      },
      "source": [
        "## Adding missing item-day combinations\n",
        "\n",
        "The aggregated data only contains entries for day-item pairs for which there is at least one transaction. This results in missing information:\n",
        "- most items are only sold on a few days; no data on days with no orders is recorded\n",
        "- there are a few items that are never sold and therefore do not appear in `orders`\n",
        "\n",
        "To account for the missing data, we add entries with `order = 0` for missing day-item combinations. This increases the number of observations from 100,771 to 1,883,340 and provides useful information about zero sales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxEosqPz9i3W",
        "outputId": "8f46e037-91c3-4af6-946f-0ec0cece6b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100771, 3)\n",
            "(1883340, 3)\n"
          ]
        }
      ],
      "source": [
        "#collapse-show\n",
        "\n",
        "# add items that were never sold before\n",
        "missing_itemIDs = set(items['itemID'].unique()) - set(orders['itemID'].unique())\n",
        "missing_rows = pd.DataFrame({'itemID':     list(missing_itemIDs), \n",
        "                            'day_of_year': np.ones(len(missing_itemIDs)).astype('int'), \n",
        "                            'order':       np.zeros(len(missing_itemIDs)).astype('int')})\n",
        "orders = pd.concat([orders, missing_rows], axis = 0)\n",
        "print(orders.shape)\n",
        "\n",
        "# add zeros for days with no transactions\n",
        "agg_orders = orders.groupby(['itemID', 'day_of_year']).order.unique().unstack('day_of_year').stack('day_of_year', dropna = False)\n",
        "agg_orders = agg_orders.reset_index()\n",
        "agg_orders.columns = ['itemID', 'day_of_year', 'order']\n",
        "agg_orders['order'].fillna(0, inplace = True)\n",
        "agg_orders['order'] = agg_orders['order'].astype(int)\n",
        "print(agg_orders.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPLbrP8l9i3W"
      },
      "source": [
        "## Labeling promotions\n",
        "\n",
        "The data documentation says that promotions in the training data are not explicitly marked.\n",
        "\n",
        "We need to manually mark promotion days. Ignoring it complicates forecasting because the number of orders in some days explodes without an apparent reason. In such cases, the underlying reason is likely a promotion, which should be reflected in a corresponding feature.\n",
        "\n",
        "We need to be very careful about marking promotions. Labeling too many days as promotions based on the number of orders risks introducing data leakage since the number of orders is unknown at the prediction time. Below, I use `find_peaks()` to isolate peaks in the `order` time series and encode them as promotions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpp0cHx09i3W",
        "outputId": "a999245d-e58d-4dcb-8487-71a2001f6545"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Daily p(promotion) per item in train: 0.0079\n",
            "Daily p(promotion) per item in test:  0.0141\n"
          ]
        }
      ],
      "source": [
        "#collapse-show\n",
        "\n",
        "# computations\n",
        "agg_orders['promotion'] = 0\n",
        "for itemID in tqdm(agg_orders['itemID'].unique()):\n",
        "    promo    = np.zeros(len(agg_orders[agg_orders['itemID'] == itemID]))\n",
        "    avg      = agg_orders[(agg_orders['itemID'] == itemID)]['order'].median()\n",
        "    std      = agg_orders[(agg_orders['itemID'] == itemID)]['order'].std()\n",
        "    peaks, _ = find_peaks(np.append(agg_orders[agg_orders['itemID'] == itemID]['order'].values, avg), # append avg to enable marking last point as promo\n",
        "                          prominence = max(5, std),  # peak difference with neighbor points; max(5,std) to exclude cases when std is too small\n",
        "                          height     = avg + 2*std)  # minimal height of a peak\n",
        "    promo[peaks] = 1\n",
        "    agg_orders.loc[agg_orders['itemID'] == itemID, 'promotion'] = promo\n",
        "\n",
        "# compare promotion number\n",
        "promo_in_train = (agg_orders['promotion'].sum() / agg_orders['day_of_year'].max()) / len(items)\n",
        "promo_in_test  = (3*len(items) - items.promotion_0.isnull().sum() - items.promotion_2.isnull().sum() - items.promotion_1.isnull().sum()) / 14 / len(items)\n",
        "print('Daily p(promotion) per item in train: {}'.format(np.round(promo_in_train, 4)))\n",
        "print('Daily p(promotion) per item in test:  {}'.format(np.round(promo_in_test , 4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGOwSe5b9i3W"
      },
      "source": [
        "Our method identifies 14,911 promotions. Compared to the test set where promotions are explicitly reported, this amounts to about half as many promos per item and day. \n",
        "\n",
        "Let's look visualize promotions for some items:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocR2REn_9i3X"
      },
      "outputs": [],
      "source": [
        "#collapse-hide\n",
        "\n",
        "# compute promo count\n",
        "promo_count = agg_orders.groupby('itemID')['promotion'].agg('sum').reset_index()\n",
        "promo_count = promo_count.sort_values('promotion').reset_index(drop = True)\n",
        "\n",
        "# plot some items\n",
        "item_plots = [0, 2000, 4000, 6000, 8000, 9000, 10000, 10100, 10200, 10300, 10400, 10462]\n",
        "fig = plt.figure(figsize = (16, 12))\n",
        "for i in range(len(item_plots)):\n",
        "    plt.subplot(3, 4, i + 1)\n",
        "    df = agg_orders[agg_orders.itemID == promo_count['itemID'][item_plots[i]]]\n",
        "    plt.scatter(df['day_of_year'], df['order'], c = df['promotion'])\n",
        "    plt.ylabel('Total Orders')\n",
        "    plt.xlabel('Day')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp6SSjd-9i3X"
      },
      "source": [
        "![](https://github.com/kozodoi/website/blob/master/_notebooks/images/fig_promotions.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45DOfIeR9i3X"
      },
      "source": [
        "The yellow marker indicates promotions. Our method identifies some outliers as promos but misses a few points that are less prominent. At the same time, we can not be sure that these cases are necessarily promotions: the large number of orders on these days could be observed due to other reasons. We will stick to this solution but note that it might require further improvement.\n",
        "\n",
        "## Feature engineering\n",
        "\n",
        "Now that the data is aggregated, we construct transaction-based features and the targets. For each day, we compute target as the total number of orders in the following 14 days. The days preceding the considered day are used to extract features. We extract slices of the past [1, 7, ..., 35] days and compute features based on data from that slice.\n",
        "\n",
        "For each item, we compute the following features:\n",
        "- total count of orders and ordered items\n",
        "- total count of promotions\n",
        "- mean item price\n",
        "- recency of the last order\n",
        "\n",
        "The number of orders and promotions is also aggregated on a manufacturer and category level.\n",
        "\n",
        "In addition, we use `tsfresh` package to automatically extract features based on the order dynamics in the last 35 days. `tsfresh` computes hundreds of features describing the time series. We only keep features with no missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pq8NJl0h9i3X",
        "outputId": "bb8c9819-b5db-4f30-fb1b-697b7fe180d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1391579, 458)\n",
            "(1391579, 470)\n"
          ]
        }
      ],
      "source": [
        "#collapse-show\n",
        "\n",
        "# packages\n",
        "from tsfresh import extract_features\n",
        "\n",
        "# parameters\n",
        "days_input  = [1, 7, 14, 21, 28, 35]\n",
        "days_target = 14\n",
        "\n",
        "# preparations\n",
        "day_first = np.max(days_input)\n",
        "day_last  = agg_orders['day_of_year'].max() - days_target + 1\n",
        "orders    = None\n",
        "\n",
        "# merge manufacturer and category\n",
        "agg_orders = agg_orders.merge(items[['itemID', 'manufacturer']], how = 'left')\n",
        "agg_orders = agg_orders.merge(items[['itemID', 'category']],     how = 'left')\n",
        "\n",
        "# computations\n",
        "for day_of_year in tqdm(list(range(149, day_last)) + [agg_orders['day_of_year'].max()]):    \n",
        "\n",
        "    ### VALIDAION: TARGET, PROMOTIONS, PRICES\n",
        "        \n",
        "    # day intervals\n",
        "    target_day_min = day_of_year + 1\n",
        "    target_day_max = day_of_year + days_target\n",
        "    \n",
        "    # compute target and promo: labeled data\n",
        "    if day_of_year < agg_orders['day_of_year'].max():\n",
        "        \n",
        "        # target and future promo\n",
        "        tmp_df = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
        "                            (agg_orders['day_of_year'] <= target_day_max)\n",
        "                           ].groupby('itemID')['order', 'promotion'].agg('sum').reset_index()\n",
        "        tmp_df.columns = ['itemID', 'target', 'promo_in_test']\n",
        "        \n",
        "        # future price\n",
        "        tmp_df['mean_price_test'] = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
        "                                               (agg_orders['day_of_year'] <= target_day_max)\n",
        "                                              ].groupby('itemID')['salesPrice'].agg('mean').reset_index()['salesPrice']\n",
        "        \n",
        "        # merge manufacturer and category\n",
        "        tmp_df = tmp_df.merge(items[['itemID', 'manufacturer', 'category']], how = 'left', on = 'itemID')\n",
        "        \n",
        "        # future price per manufacturer\n",
        "        tmp_df_manufacturer = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
        "                                         (agg_orders['day_of_year'] <= target_day_max)\n",
        "                                         ].groupby('manufacturer')['salesPrice'].agg('mean').reset_index()\n",
        "        tmp_df_manufacturer.columns = ['manufacturer', 'mean_price_test_manufacturer']\n",
        "        tmp_df = tmp_df.merge(tmp_df_manufacturer, how = 'left', on = 'manufacturer')\n",
        "        \n",
        "        # future price per category\n",
        "        tmp_df_category = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
        "                                     (agg_orders['day_of_year'] <= target_day_max)\n",
        "                                     ].groupby('category')['salesPrice'].agg('mean').reset_index()\n",
        "        tmp_df_category.columns = ['category', 'mean_price_test_category']\n",
        "        tmp_df = tmp_df.merge(tmp_df_category, how = 'left', on = 'category')\n",
        "        \n",
        "        # future promo per manufacturer\n",
        "        tmp_df_manufacturer = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
        "                                         (agg_orders['day_of_year'] <= target_day_max)\n",
        "                                         ].groupby('manufacturer')['promotion'].agg('sum').reset_index()\n",
        "        tmp_df_manufacturer.columns = ['manufacturer', 'promo_in_test_manufacturer']\n",
        "        tmp_df = tmp_df.merge(tmp_df_manufacturer, how = 'left', on = 'manufacturer')\n",
        "\n",
        "        # future promo per category\n",
        "        tmp_df_category = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
        "                                     (agg_orders['day_of_year'] <= target_day_max)\n",
        "                                     ].groupby('category')['promotion'].agg('sum').reset_index()\n",
        "        tmp_df_category.columns = ['category', 'promo_in_test_category']\n",
        "        tmp_df = tmp_df.merge(tmp_df_category, how = 'left', on = 'category')\n",
        "                       \n",
        "        \n",
        "    # compute target and promo: unlabeled data\n",
        "    else:\n",
        "        \n",
        "        # placeholders\n",
        "        tmp_df = pd.DataFrame({'itemID':                     items.itemID,\n",
        "                               'target':                     np.nan,\n",
        "                               'promo_in_test':              np.nan,\n",
        "                               'mean_price_test':            items.simulationPrice,\n",
        "                               'manufacturer':               items.manufacturer,\n",
        "                               'category':                   items.category,\n",
        "                               'promo_in_test_manufacturer': np.nan,\n",
        "                               'promo_in_test_category':     np.nan})\n",
        "\n",
        "        \n",
        "    ### TRAINING: LAG-BASED FEATURES\n",
        "            \n",
        "    # compute features\n",
        "    for day_input in days_input:\n",
        "        \n",
        "        # day intervals\n",
        "        input_day_min  = day_of_year - day_input + 1\n",
        "        input_day_max  = day_of_year\n",
        "    \n",
        "        # frequency, promo and price\n",
        "        tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
        "                                  (agg_orders['day_of_year'] <= input_day_max)\n",
        "                                 ].groupby('itemID')\n",
        "        tmp_df['order_sum_last_'   + str(day_input)] = tmp_df_input['order'].agg('sum').reset_index()['order']\n",
        "        tmp_df['order_count_last_' + str(day_input)] = tmp_df_input['order'].agg(lambda x: len(x[x > 0])).reset_index()['order']\n",
        "        tmp_df['promo_count_last_' + str(day_input)] = tmp_df_input['promotion'].agg('sum').reset_index()['promotion']\n",
        "        tmp_df['mean_price_last_'  + str(day_input)] = tmp_df_input['salesPrice'].agg('mean').reset_index()['salesPrice']\n",
        "\n",
        "        # frequency, promo per manufacturer\n",
        "        tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
        "                                  (agg_orders['day_of_year'] <= input_day_max)\n",
        "                                 ].groupby('manufacturer')\n",
        "        tmp_df_manufacturer = tmp_df_input['order'].agg('sum').reset_index()\n",
        "        tmp_df_manufacturer.columns = ['manufacturer', 'order_manufacturer_sum_last_' + str(day_input)]\n",
        "        tmp_df_manufacturer['order_manufacturer_count_last_' + str(day_input)] = tmp_df_input['order'].agg(lambda x: len(x[x > 0])).reset_index()['order']\n",
        "        tmp_df_manufacturer['promo_manufacturer_count_last_' + str(day_input)] = tmp_df_input['promotion'].agg('sum').reset_index()['promotion']\n",
        "        tmp_df = tmp_df.merge(tmp_df_manufacturer, how = 'left', on = 'manufacturer')\n",
        "    \n",
        "        # frequency, promo per category\n",
        "        tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
        "                                  (agg_orders['day_of_year'] <= input_day_max)\n",
        "                                 ].groupby('category')\n",
        "        tmp_df_category = tmp_df_input['order'].agg('sum').reset_index()\n",
        "        tmp_df_category.columns = ['category', 'order_category_sum_last_' + str(day_input)]       \n",
        "        tmp_df_category['order_category_count_last_' + str(day_input)] = tmp_df_input['order'].agg(lambda x: len(x[x > 0])).reset_index()['order']\n",
        "        tmp_df_category['promo_category_count_last_' + str(day_input)] = tmp_df_input['promotion'].agg('sum').reset_index()['promotion']\n",
        "        tmp_df = tmp_df.merge(tmp_df_category, how = 'left', on = 'category')\n",
        "\n",
        "        # frequency, promo per all items\n",
        "        tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
        "                                  (agg_orders['day_of_year'] <= input_day_max)]\n",
        "        tmp_df['order_all_sum_last_'   + str(day_input)] = tmp_df_input['order'].agg('sum')\n",
        "        tmp_df['order_all_count_last_' + str(day_input)] = tmp_df_input['order'].agg(lambda x: len(x[x > 0]))\n",
        "        tmp_df['promo_all_count_last_' + str(day_input)] = tmp_df_input['promotion'].agg('sum')\n",
        "        \n",
        "        # recency\n",
        "        if day_input == max(days_input):\n",
        "            tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
        "                                      (agg_orders['day_of_year'] <= input_day_max) &\n",
        "                                      (agg_orders['order'] > 0)\n",
        "                                     ].groupby('itemID')\n",
        "            tmp_df['days_since_last_order'] = (day_of_year - tmp_df_input['day_of_year'].agg('max')).reindex(tmp_df.itemID).reset_index()['day_of_year']\n",
        "            tmp_df['days_since_last_order'].fillna(day_input, inplace = True)\n",
        "            \n",
        "            \n",
        "        # tsfresh features\n",
        "        if day_input == max(days_input):\n",
        "            tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
        "                                      (agg_orders['day_of_year'] <= input_day_max)]\n",
        "            tmp_df_input = tmp_df_input[['day_of_year', 'itemID', 'order']]\n",
        "            extracted_features = extract_features(tmp_df_input, column_id = 'itemID', column_sort = 'day_of_year')\n",
        "            extracted_features['itemID'] = extracted_features.index\n",
        "            tmp_df = tmp_df.merge(extracted_features, how = 'left', on = 'itemID')\n",
        "            \n",
        "            \n",
        "    ### FINAL PREPARATIONS\n",
        "            \n",
        "    # day of year\n",
        "    tmp_df.insert(1, column = 'day_of_year', value = day_of_year)\n",
        "        \n",
        "    # merge data\n",
        "    orders = pd.concat([orders, tmp_df], axis = 0)\n",
        "    \n",
        "    # drop manufacturer and category\n",
        "    del orders['manufacturer']\n",
        "    del orders['category']\n",
        "\n",
        "\n",
        "##### REMOVE MISSINGS\n",
        "\n",
        "good_nas = ['target', \n",
        "            'mean_price_test_category', 'mean_price_test_manufacturer',\n",
        "            'promo_in_test', 'promo_in_test_category', 'promo_in_test_manufacturer']\n",
        "nonas = list(orders.columns[orders.isnull().sum() == 0]) + good_nas\n",
        "orders = orders[nonas]\n",
        "print(orders.shape)\n",
        "\n",
        "\n",
        "##### COMPUTE MEAN PRICE RATIOS\n",
        "\n",
        "print(orders.shape)\n",
        "price_vars = ['mean_price_last_1', 'mean_price_last_7', 'mean_price_last_14', \n",
        "              'mean_price_last_21', 'mean_price_last_28', 'mean_price_last_35']\n",
        "for var in price_vars:\n",
        "    orders['ratio_'              + str(var)] = orders['mean_price_test']              / orders[var]\n",
        "    orders['ratio_manufacturer_' + str(var)] = orders['mean_price_test_manufacturer'] / orders[var]\n",
        "    orders['ratio_category_'     + str(var)] = orders['mean_price_test_category']     / orders[var]\n",
        "print(orders.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF6Dfitz9i3Y"
      },
      "source": [
        "The feature extraction takes about ten hours and outputs a data set with 470 features. Great job!\n",
        "\n",
        "Now, let's create features in the `items` data set:\n",
        "- ratio of the actual and recommended price\n",
        "- item category index constructed of three subcategories\n",
        "- customer rating relative to the average rating of the items of the same manufacturer or category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYdLJ8ZG9i3Y"
      },
      "outputs": [],
      "source": [
        "#collapse-show\n",
        "\n",
        "# price ratio\n",
        "items['recommended_simulation_price_ratio'] = items['simulationPrice'] / items['recommendedRetailPrice']\n",
        "\n",
        "# detailed item category\n",
        "items['category'] = items['category1'].astype(str) + items['category2'].astype(str) + items['category3'].astype(str)\n",
        "items['category'] = items['category'].astype(int)\n",
        "\n",
        "# customer rating ratio per manufacturer\n",
        "rating_manufacturer = items.groupby('manufacturer')['customerRating'].agg('mean').reset_index()\n",
        "rating_manufacturer.columns = ['manufacturer', 'mean_customerRating_manufacturer']\n",
        "items = items.merge(rating_manufacturer, how = 'left', on = 'manufacturer')\n",
        "items['customerRating_manufacturer_ratio'] = items['customerRating'] / items['mean_customerRating_manufacturer']\n",
        "del items['mean_customerRating_manufacturer']\n",
        "\n",
        "# customer rating ratio per category\n",
        "rating_category = items.groupby('category')['customerRating'].agg('mean').reset_index()\n",
        "rating_category.columns = ['category', 'mean_customerRating_category']\n",
        "items = items.merge(rating_category, how = 'left', on = 'category')\n",
        "items['customerRating_category_ratio'] = items['customerRating'] / items['mean_customerRating_category']\n",
        "del items['mean_customerRating_category']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1aF0EUS9i3Y"
      },
      "source": [
        "We can now merge `orders` and `items`. We also partition the data into the labeled training set and the unlabeled test set, compute some missing features for the test set and export the data as csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc4Hu5Tp9i3Y",
        "outputId": "ec9d17a8-f216-4322-e6ca-cf3761e61b8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved as ../data/prepared/df_v14.csv\n",
            "Saved as ../data/prepared/df_test_v14.csv\n",
            "(1381116, 476)\n",
            "(10463, 476)\n"
          ]
        }
      ],
      "source": [
        "#collapse-hide\n",
        "\n",
        "##### DATA PARTITIONING\n",
        "\n",
        "# merge data\n",
        "df = pd.merge(orders, items, on = 'itemID', how = 'left')\n",
        "\n",
        "# partition intro train and test\n",
        "df_train = df[df['day_of_year'] <  df['day_of_year'].max()]\n",
        "df_test  = df[df['day_of_year'] == df['day_of_year'].max()]\n",
        "\n",
        "\n",
        "##### COMPUTE FEATURES FOR TEST DATA\n",
        "\n",
        "# add promotion info to test\n",
        "promo_vars = df_test.filter(like = 'promotion_').columns\n",
        "df_test['promo_in_test'] = 3 - df_test[promo_vars].isnull().sum(axis = 1)\n",
        "df_test['promo_in_test'].describe()\n",
        "\n",
        "del df_test['promo_in_test_manufacturer'], df_test['promo_in_test_category']\n",
        "\n",
        "# future promo per manufacturer\n",
        "tmp_df_manufacturer = df_test.groupby('manufacturer')['promo_in_test'].agg('sum').reset_index()\n",
        "tmp_df_manufacturer.columns = ['manufacturer', 'promo_in_test_manufacturer']\n",
        "df_test = df_test.merge(tmp_df_manufacturer, how = 'left', on = 'manufacturer')\n",
        "\n",
        "# future promo per category\n",
        "tmp_df_category = df_test.groupby('category')['promo_in_test'].agg('sum').reset_index()\n",
        "tmp_df_category.columns = ['category', 'promo_in_test_category']\n",
        "df_test = df_test.merge(tmp_df_category, how = 'left', on = 'category')\n",
        "\n",
        "del df_test['mean_price_test_manufacturer'], df_test['mean_price_test_category']\n",
        "\n",
        "# future price per manufacturer\n",
        "tmp_df_manufacturer = df_test.groupby('manufacturer')['mean_price_test'].agg('mean').reset_index()\n",
        "tmp_df_manufacturer.columns = ['manufacturer', 'mean_price_test_manufacturer']\n",
        "df_test = df_test.merge(tmp_df_manufacturer, how = 'left', on = 'manufacturer')\n",
        "\n",
        "# future price per category\n",
        "tmp_df_category = df_test.groupby('category')['mean_price_test'].agg('mean').reset_index()\n",
        "tmp_df_category.columns = ['category', 'mean_price_test_category']\n",
        "df_test = df_test.merge(tmp_df_category, how = 'left', on = 'category')\n",
        "\n",
        "# mean price ratios\n",
        "for var in price_vars:\n",
        "    df_test['ratio_'              + str(var)] = df_test['mean_price_test']              / df_test[var]\n",
        "    df_test['ratio_manufacturer_' + str(var)] = df_test['mean_price_test_manufacturer'] / df_test[var]\n",
        "    df_test['ratio_category_'     + str(var)] = df_test['mean_price_test_category']     / df_test[var]\n",
        "\n",
        "\n",
        "##### DROP FEATURES\n",
        "\n",
        "# drop promotion dates\n",
        "df_test.drop(promo_vars,  axis = 1, inplace = True)\n",
        "df_train.drop(promo_vars, axis = 1, inplace = True)\n",
        "\n",
        "# drop mean prices\n",
        "price_vars = price_vars + ['mean_price_test_manufacturer', 'mean_price_test_category']\n",
        "df_test.drop(price_vars,  axis = 1, inplace = True)\n",
        "df_train.drop(price_vars, axis = 1, inplace = True)\n",
        "\n",
        "# export data\n",
        "save_csv_version('../data/prepared/df.csv',      df_train, index = False, compression = 'gzip')\n",
        "save_csv_version('../data/prepared/df_test.csv', df_test,  index = False, compression = 'gzip', min_version = 3)\n",
        "print(df_train.shape)\n",
        "print(df_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ1Wj4q_9i3Z"
      },
      "source": [
        "# 4. Modeling\n",
        "\n",
        "## Custom loss functions\n",
        "\n",
        "Machine learning encompasses a wide range of statically-inspired performance metrics such as MSE, MAE and others. In practice, machine learning models are used by a company that has specific goals. Usually, these goals can not be expressed in terms of such simple metrics. Therefore, it is important to come up with an evaluation metric consistent with the company's objectives to ensure that we judge performance on a criterion that actually matters.\n",
        "\n",
        "In the DMC 2020 task, we are given a profit function of the retailer. The function accounts for asymmetric error costs: underpredicting demand results in lost revenue because the retailer can not sell a product that is not ready to ship, whereas overpredicting demand incurs a fee for storing the excessive amount of product. \n",
        "\n",
        "Below, we derive profit according to the task description:\n",
        "\n",
        "![](https://github.com/kozodoi/website/blob/master/_notebooks/images/fig_profit.png?raw=1)\n",
        "\n",
        "Let's implement the profit function in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrr03nKi9i3Z"
      },
      "outputs": [],
      "source": [
        "def profit(y_true, y_pred, price):\n",
        "    '''\n",
        "    Computes retailer's profit.\n",
        "    \n",
        "    Arguments:\n",
        "    - y_true (numpy array): ground truth demand values\n",
        "    - y_pred (numpy array): predicted demand values\n",
        "    - price (numpy array): item prices\n",
        "\n",
        "    Returns:\n",
        "    - profit value\n",
        "    '''\n",
        "\n",
        "    # remove negative and round\n",
        "    y_pred = np.where(y_pred > 0, y_pred, 0)\n",
        "    y_pred = np.round(y_pred).astype('int')\n",
        "\n",
        "    # sold units\n",
        "    units_sold = np.minimum(y_true, y_pred)\n",
        "\n",
        "    # overstocked units\n",
        "    units_overstock = y_pred - y_true\n",
        "    units_overstock[units_overstock < 0] = 0\n",
        "\n",
        "    # profit\n",
        "    revenue = units_sold * price\n",
        "    fee     = units_overstock * price * 0.6\n",
        "    profit  = revenue - fee\n",
        "    profit  = profit.sum()\n",
        "    \n",
        "    return profit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SYn_LDE9i3Z"
      },
      "source": [
        "The function above is great for evaluating quality of our predictions. But can we directly optimize it during modeling? \n",
        "\n",
        "LightGBM supports custom loss functions on both training and validation stages. To use a custom loss during training, one needs to supply a function with its first and second-order derivatives. \n",
        "\n",
        "Ideally, we would like to define the loss as a difference between the potential profit (given our demand prediction) and the oracle profit (when demand prediction is correct). However, such a loss is not differentiable. We can not compute derivatives to plug it as a training loss. Instead, we need to come up with a slightly different function that approximates profit and satisfies the loss conditions. \n",
        "\n",
        "We define the loss as a squared difference between the oracle profit and profit based on predicted demand. In this setting, we can compute loss derivatives with respect to the prediction (Gradient and Hessian):\n",
        "\n",
        "![](https://github.com/kozodoi/website/blob/master/_notebooks/images/fig_loss.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV7lwC5C9i3Z"
      },
      "source": [
        "The snippet below implements the training and validation losses for LightGBM. You can notice that we do not include the squared prices in the loss functions. The reason is that with `sklearn` API, it is difficult to include external variables like prices in the loss, so we will include the price later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKmOu8Oy9i3Z"
      },
      "outputs": [],
      "source": [
        "# collpase-show\n",
        "\n",
        "##### TRAINING LOSS\n",
        "def asymmetric_mse(y_true, y_pred):\n",
        "    '''\n",
        "    Asymmetric MSE objective for training LightGBM regressor.\n",
        "     \n",
        "    Arguments:\n",
        "    - y_true (numpy array): ground truth target values\n",
        "    - y_pred (numpy array): estimated target values\n",
        "    \n",
        "    Returns:\n",
        "    - gradient\n",
        "    - hessian\n",
        "    '''\n",
        "    \n",
        "    residual = (y_true - y_pred).astype('float')    \n",
        "    grad     = np.where(residual > 0, -2*residual, -0.72*residual)\n",
        "    hess     = np.where(residual > 0,  2.0, 0.72)\n",
        "    return grad, hess\n",
        "\n",
        "\n",
        "##### VALIDATION LOSS\n",
        "def asymmetric_mse_eval(y_true, y_pred):\n",
        "    \n",
        "    '''\n",
        "    Asymmetric MSE evaluation metric for evaluating LightGBM regressor.\n",
        "     \n",
        "    Arguments:\n",
        "    - y_true (numpy array): ground truth target values\n",
        "    - y_pred (numpy array): estimated target values\n",
        "    \n",
        "    Returns:\n",
        "    - metric name\n",
        "    - metric value\n",
        "    - whether the metric is maximized\n",
        "    '''\n",
        "    \n",
        "    residual = (y_true - y_pred).astype('float')      \n",
        "    loss     = np.where(residual > 0, 2*residual**2, 0.72*residual**2)\n",
        "    return 'asymmetric_mse_eval', np.mean(loss), False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxW_63gj9i3Z"
      },
      "source": [
        "How to deal with the prices? \n",
        "\n",
        "One option is to account for them within the `fit()` method. LightGBM supports weighing observations using the arguments `sample_weight` and `eval_sample_weight`. You will see how we supply price vectors in the modeling code in the next section. Note that including prices as weights instead of plugging them into the loss leads to losing some information, since Gradients and Hessians are computed without the price multiplication. Still, this approach provides a pretty close approximation of the original profit function. If you are interested in including prices in the loss, you can check `lightGBM` API that allows more flexibility.\n",
        "\n",
        "The only missing piece is the relationship between the penalty size and the prediction error. By taking a square root of the profit differences instead of the absolute value, we penalize larger errors more than the smaller ones. However, our profit changes linearly with the error size. This is how we can can address it:\n",
        "- transform target using a non-linear transformation (e.g. square root)\n",
        "- train a model that optimizes the MSE loss on the transformed target \n",
        "- apply the inverse transformation to the model predictions\n",
        "\n",
        "Target transformation smooths out the square effect in MSE. We still penalize large errors more, but the large errors on a transformed scale are also smaller compared to the original scale. This helps to balance the two effects and approximate a linear relationship between the error size and the loss penalty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6agt4NMv9i3a"
      },
      "source": [
        "## Modeling pipeline\n",
        "\n",
        "Good, let's start building models! First, we extract the target and flag ID features not used for prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq-XydV49i3a",
        "outputId": "db6c11cb-0177-421f-cfb1-4cdad020fe0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1381116, 475) (1381116,)\n",
            "(10463, 475)\n"
          ]
        }
      ],
      "source": [
        "#collapse-hide\n",
        "\n",
        "# extract target\n",
        "y = df_train['target']\n",
        "X = df_train.drop('target', axis = 1)\n",
        "del df_train\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "# format test data\n",
        "X_test = df_test.drop('target', axis = 1)\n",
        "del df_test\n",
        "print(X_test.shape)\n",
        "\n",
        "# relevant features\n",
        "drop_feats = ['itemID', 'day_of_year']\n",
        "features = [var for var in X.columns if var not in drop_feats]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yStAFIIl9i3a"
      },
      "source": [
        "The modeling pipeline uses multiple tricks discovered during the model refinement process. We toggle these tricks using logical variables that define the following training options:\n",
        "- `target_transform = True`: transforms target to reduce penalty for large errors. Motivation for this is provided in the previous section.\n",
        "- `train_on_positive = False`: trains only on cases with positive sales (i.e., at least one of the order lags is greater than zero) and predicts null demand for items with no sales. This substantially reduces the training time but also leads to a drop in the performance.\n",
        "- `two_stage = True`: trains a two-stage model: (i) binary classifier predicting whether the future sales will be zero; (ii) regression model predicting the volume of sales. Predictions of the regression model are only stored for cases where the classifier predicts positive sales.\n",
        "- `tuned_params = True`: imports optimized LightGBM hyper-parameter values. The next section describes the tuning procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc2toAkc9i3a"
      },
      "outputs": [],
      "source": [
        "#collapse-hide\n",
        "\n",
        "##### TRAINING OPTIONS\n",
        "\n",
        "# target transformation\n",
        "target_transform = True\n",
        "\n",
        "# train on positive sales only\n",
        "train_on_positive = False\n",
        "\n",
        "# two-stage model\n",
        "two_stage = True\n",
        "\n",
        "# use tuned meta-params\n",
        "tuned_params = True\n",
        "\n",
        "\n",
        "##### CLASSIFIER PARAMETERS\n",
        "\n",
        "# rounds and options\n",
        "cores       = 4\n",
        "stop_rounds = 100\n",
        "verbose     = 100\n",
        "seed        = 23\n",
        "\n",
        "# LGB parameters\n",
        "lgb_params = {\n",
        "    'boosting_type':    'goss',\n",
        "    'objective':        asymmetric_mse,\n",
        "    'metrics':          asymmetric_mse_eval,\n",
        "    'n_estimators':     1000,\n",
        "    'learning_rate':    0.1,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'feature_fraction': 0.8,\n",
        "    'lambda_l1':        0.1,\n",
        "    'lambda_l2':        0.1,\n",
        "    'silent':           True,\n",
        "    'verbosity':        -1,\n",
        "    'nthread' :         cores,\n",
        "    'random_state':     seed,\n",
        "}\n",
        "\n",
        "# load optimal parameters\n",
        "if tuned_params:\n",
        "    par_file   = open('../lgb_meta_params_100.pkl', 'rb')\n",
        "    lgb_params = pickle.load(par_file)\n",
        "    lgb_params['nthread']      = cores\n",
        "    lgb_params['random_state'] = seed\n",
        "\n",
        "# second-stage LGB\n",
        "if two_stage:\n",
        "    lgb_classifier_params              = lgb_params.copy()\n",
        "    lgb_classifier_params['objective'] = 'binary'\n",
        "    lgb_classifier_params['metrics']   = 'logloss'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml1Lx-Hn9i3a"
      },
      "source": [
        "We also define the partitioning parameters. We use a sliding window approach with 7 folds, where each subsequent fold is shifted by one day into the past. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drl-Vssv9i3a"
      },
      "outputs": [],
      "source": [
        "#collapse-hide\n",
        "num_folds = 7   # no. CV folds\n",
        "test_days = 14  # no. days in the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP1bMHim9i3a"
      },
      "source": [
        "\n",
        "\n",
        "Let's explain the partitioning using the first fold as an example. Each fold is divided into training and validation subsets. The first 35 days are cut off and only used to compute lag-based features for the days starting from 36. Days 36 - 145 are used for training. For each of these days, we have features based on the previous 35 days and targets based on the next 14 days. Days 159 - 173 are used for validation. Days 146 - 158 between training and validation subsets are skipped to avoid data leakage: the target for these days would use information from the validation period.\n",
        "\n",
        "![](https://github.com/kozodoi/website/blob/master/_notebooks/images/fig_partitioning.png?raw=1)\n",
        "\n",
        "We can now set up a modeling loop with the following steps for each of the folds:\n",
        "- extract data from the fold and partition it into training and validation sets\n",
        "- train LightGBM on the training set and perform early stopping on the validation set\n",
        "- save predictions for the validation set (denoted as OOF) and predictions for the test set\n",
        "- save feature importance and performance on the validation fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTZJKIsc9i3b",
        "outputId": "f189c0f4-649a-46a2-cb9a-6e3d1f259be3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------\n",
            "- train period days: 41 -- 151 (n = 1161393)\n",
            "- valid period days: 166 -- 166 (n = 10463)\n",
            "-----------------------------------------------------------------\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[532]\ttraining's binary_logloss: 0.238417\tvalid_1's binary_logloss: 0.347182\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[788]\ttraining's rmse: 0.611924\ttraining's asymmetric_mse_eval: 2.82734\tvalid_1's rmse: 0.98004\tvalid_1's asymmetric_mse_eval: 5.83945\n",
            "-----------------------------------------------------------------\n",
            "FOLD 1/7: RMSE = 74.46, PROFIT = 4146664\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "...\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "- train period days: 35 -- 145 (n = 1161393)\n",
            "- valid period days: 160 -- 160 (n = 10463)\n",
            "-----------------------------------------------------------------\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[711]\ttraining's binary_logloss: 0.22193\tvalid_1's binary_logloss: 0.338637\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[782]\ttraining's rmse: 0.600011\ttraining's asymmetric_mse_eval: 2.71385\tvalid_1's rmse: 0.987073\tvalid_1's asymmetric_mse_eval: 5.67223\n",
            "-----------------------------------------------------------------\n",
            "FOLD 7/7: RMSE = 74.46, PROFIT = 3958098\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "- AVERAGE RMSE:   69.66\n",
            "- AVERAGE PROFIT: 3997598 (54.19%)\n",
            "- RUNNING TIME:   88.53 minutes\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#collapse-show\n",
        "\n",
        "# placeholders\n",
        "importances   = pd.DataFrame()\n",
        "preds_oof     = np.zeros((num_folds, items.shape[0]))\n",
        "reals_oof     = np.zeros((num_folds, items.shape[0]))\n",
        "prices_oof    = np.zeros((num_folds, items.shape[0]))\n",
        "preds_test    = np.zeros(items.shape[0])\n",
        "oof_rmse      = []\n",
        "oof_profit    = []\n",
        "oracle_profit = []\n",
        "clfs          = []\n",
        "train_idx     = []\n",
        "valid_idx     = []\n",
        "\n",
        "# objects\n",
        "train_days = X['day_of_year'].max() - test_days + 1 - num_folds - X['day_of_year'].min() # no. days in the train set\n",
        "time_start = time.time()\n",
        "\n",
        "# modeling loop\n",
        "for fold in range(num_folds):\n",
        "    \n",
        "    ##### PARTITIONING\n",
        "    \n",
        "    # dates\n",
        "    if fold == 0:\n",
        "        v_end = X['day_of_year'].max()\n",
        "    else:\n",
        "        v_end = v_end - 1\n",
        "    v_start = v_end\n",
        "    t_end   = v_start - (test_days + 1)\n",
        "    t_start = t_end   - (train_days - 1)\n",
        "    \n",
        "    # extract index\n",
        "    train_idx.append(list(X[(X.day_of_year >= t_start) & (X.day_of_year <= t_end)].index))\n",
        "    valid_idx.append(list(X[(X.day_of_year >= v_start) & (X.day_of_year <= v_end)].index))   \n",
        "    \n",
        "    # extract samples\n",
        "    X_train, y_train = X.iloc[train_idx[fold]][features], y.iloc[train_idx[fold]]\n",
        "    X_valid, y_valid = X.iloc[valid_idx[fold]][features], y.iloc[valid_idx[fold]]\n",
        "    X_test = X_test[features]\n",
        "    \n",
        "    # keep positive cases\n",
        "    if train_on_positive:\n",
        "        y_train = y_train.loc[(X_train['order_sum_last_28'] > 0) | (X_train['promo_in_test'] > 0)]\n",
        "        X_train = X_train.loc[(X_train['order_sum_last_28'] > 0) | (X_train['promo_in_test'] > 0)]\n",
        "\n",
        "    # information\n",
        "    print('-' * 65)\n",
        "    print('- train period days: {} -- {} (n = {})'.format(t_start, t_end, len(train_idx[fold])))\n",
        "    print('- valid period days: {} -- {} (n = {})'.format(v_start, v_end, len(valid_idx[fold])))\n",
        "    print('-' * 65)\n",
        "\n",
        "    \n",
        "    ##### MODELING\n",
        "    \n",
        "    # target transformation\n",
        "    if target_transform:\n",
        "        y_train = np.sqrt(y_train)\n",
        "        y_valid = np.sqrt(y_valid)\n",
        "        \n",
        "    # first stage model\n",
        "    if two_stage:\n",
        "        y_train_binary, y_valid_binary = y_train.copy(), y_valid.copy()\n",
        "        y_train_binary[y_train_binary > 0] = 1\n",
        "        y_valid_binary[y_valid_binary > 0] = 1\n",
        "        clf_classifier = lgb.LGBMClassifier(**lgb_classifier_params) \n",
        "        clf_classifier = clf_classifier.fit(X_train, y_train_binary, \n",
        "                                            eval_set              = [(X_train, y_train_binary), (X_valid, y_valid_binary)],\n",
        "                                            eval_metric           = 'logloss',\n",
        "                                            early_stopping_rounds = stop_rounds,\n",
        "                                            verbose               = verbose)\n",
        "        preds_oof_fold_binary  = clf_classifier.predict(X_valid)\n",
        "        preds_test_fold_binary = clf_classifier.predict(X_test)\n",
        "\n",
        "    # training\n",
        "    clf = lgb.LGBMRegressor(**lgb_params) \n",
        "    clf = clf.fit(X_train, y_train, \n",
        "                  eval_set              = [(X_train, y_train), (X_valid, y_valid)], \n",
        "                  eval_metric           = asymmetric_mse_eval,\n",
        "                  sample_weight         = X_train['simulationPrice'].values,\n",
        "                  eval_sample_weight    = [X_train['simulationPrice'].values, X_valid['simulationPrice'].values],\n",
        "                  early_stopping_rounds = stop_rounds,\n",
        "                  verbose               = verbose)\n",
        "    clfs.append(clf)\n",
        "    \n",
        "    # inference\n",
        "    if target_transform:      \n",
        "        preds_oof_fold  = postprocess_preds(clf.predict(X_valid)**2)\n",
        "        reals_oof_fold  = y_valid**2\n",
        "        preds_test_fold = postprocess_preds(clf.predict(X_test)**2) / num_folds\n",
        "    else:\n",
        "        preds_oof_fold  = postprocess_preds(clf.predict(X_valid))\n",
        "        reals_oof_fold  = y_valid\n",
        "        preds_test_fold = postprocess_preds(clf.predict(X_test)) / num_folds\n",
        "        \n",
        "    # impute zeros\n",
        "    if train_on_positive:\n",
        "        preds_oof_fold[(X_valid['order_sum_last_28'] == 0) & (X_valid['promo_in_test'] == 0)] = 0\n",
        "        preds_test_fold[(X_test['order_sum_last_28'] == 0) & (X_test['promo_in_test']  == 0)] = 0\n",
        "        \n",
        "    # multiply with first stage predictions\n",
        "    if two_stage:\n",
        "        preds_oof_fold  = preds_oof_fold  * np.round(preds_oof_fold_binary)\n",
        "        preds_test_fold = preds_test_fold * np.round(preds_test_fold_binary)\n",
        "\n",
        "    # write predictions\n",
        "    preds_oof[fold, :] = preds_oof_fold\n",
        "    reals_oof[fold, :] = reals_oof_fold\n",
        "    preds_test        += preds_test_fold\n",
        "    \n",
        "    # save prices\n",
        "    prices_oof[fold, :] = X.iloc[valid_idx[fold]]['simulationPrice'].values\n",
        "        \n",
        "        \n",
        "    ##### EVALUATION\n",
        "\n",
        "    # evaluation\n",
        "    oof_rmse.append(np.sqrt(mean_squared_error(reals_oof[fold, :], \n",
        "                                               preds_oof[fold, :])))\n",
        "    oof_profit.append(profit(reals_oof[fold, :], \n",
        "                             preds_oof[fold, :], \n",
        "                             price = X.iloc[valid_idx[fold]]['simulationPrice'].values))\n",
        "    oracle_profit.append(profit(reals_oof[fold, :], \n",
        "                                reals_oof[fold, :], \n",
        "                                price = X.iloc[valid_idx[fold]]['simulationPrice'].values))\n",
        "    \n",
        "    # feature importance\n",
        "    fold_importance_df = pd.DataFrame()\n",
        "    fold_importance_df['Feature'] = features\n",
        "    fold_importance_df['Importance'] = clf.feature_importances_\n",
        "    fold_importance_df['Fold'] = fold + 1\n",
        "    importances = pd.concat([importances, fold_importance_df], axis = 0)\n",
        "    \n",
        "    # information\n",
        "    print('-' * 65)\n",
        "    print('FOLD {:d}/{:d}: RMSE = {:.2f}, PROFIT = {:.0f}'.format(fold + 1, \n",
        "                                                                  num_folds, \n",
        "                                                                  oof_rmse[fold], \n",
        "                                                                  oof_profit[fold]))\n",
        "    print('-' * 65)\n",
        "    print('')\n",
        "    \n",
        "\n",
        "# print performance\n",
        "print('')\n",
        "print('-' * 65)\n",
        "print('- AVERAGE RMSE:   {:.2f}'.format(np.mean(oof_rmse)))\n",
        "print('- AVERAGE PROFIT: {:.0f} ({:.2f}%)'.format(np.mean(oof_profit), 100 * np.mean(oof_profit) / np.mean(oracle_profit)))\n",
        "print('- RUNNING TIME:   {:.2f} minutes'.format((time.time() - time_start) / 60))\n",
        "print('-' * 65)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8CDDqc29i3b"
      },
      "source": [
        "Looks good! The modeling pipeline took us about 1.5 hours to run. \n",
        "\n",
        "Forecasting demand with our models results in 3,997,598 daily profit, which is about 54% of the maximum possible profit. Let's visualize the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyFkKxXw9i3b"
      },
      "outputs": [],
      "source": [
        "# collapse-hide\n",
        "\n",
        "fig = plt.figure(figsize = (20, 7))\n",
        "\n",
        "# residual plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(reals_oof.reshape(-1), preds_oof.reshape(-1))\n",
        "axis_lim = np.max([reals_oof.max(), preds_oof.max()])\n",
        "plt.ylim(top   = 1.02*axis_lim)\n",
        "plt.xlim(right = 1.02*axis_lim)\n",
        "plt.plot((0, axis_lim), (0, axis_lim), 'r--')\n",
        "plt.title('Residual Plot')\n",
        "plt.ylabel('Predicted demand')\n",
        "plt.xlabel('Actual demand')\n",
        "\n",
        "# feature importance\n",
        "plt.subplot(1, 2, 2)\n",
        "top_feats = 50\n",
        "cols = importances[['Feature', 'Importance']].groupby('Feature').mean().sort_values(by = 'Importance', ascending = False)[0:top_feats].index\n",
        "importance = importances.loc[importances.Feature.isin(cols)]\n",
        "sns.barplot(x = 'Importance', y = 'Feature', data = importance.sort_values(by = 'Importance', ascending = False), ci = 0)\n",
        "plt.title('Feature Importance')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4q-ceM-9i3b"
      },
      "source": [
        "![](https://github.com/kozodoi/website/blob/master/_notebooks/images/fig_lgb_perf.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3r9AWLo9i3b"
      },
      "source": [
        "The scatterplot shows that there is a space for further improvement: many predictions are far from the 45-degree line where predicted and real orders are equal. The important features mostly contain price information followed by features that count the previous orders.\n",
        "\n",
        "We can now use predictions stored in `preds_test` to create a submission. Mission accomplished!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7KAKu4L9i3b"
      },
      "source": [
        "# 5. Hyper-parameter tuning\n",
        "\n",
        "One way to improve our solution is to optimize the LightGBM hyper-parameters.\n",
        "\n",
        "We tune hyper-parameters using the `hyperopt` package, which performs optimization using Tree of Parzen Estimators (TPE) as a search algorithm. You don't really need to know how TPE works. As a user, you are only required to supply a parameter grid indicating the range of possible values. Compared to standard tuning methods like grid search or random search, TPE explores the search space more efficiently, allowing you to find a suitable solution faster. If you want to read more, see the package documentation [here](http://hyperopt.github.io/hyperopt/). \n",
        "\n",
        "So, let's specify hyper-parameter ranges! We create a dictionary using the following options:\n",
        "- `hp.choice('name', list_of_values)`: sets a hyper-parameter to one of the values from a list. This is suitable for hyper-parameters that can have multiple distinct values like `boosting_type`\n",
        "- `hp.uniform('name', min, max)`: sets a hyper-parameter to a float between `min` and `max`. This works well with hyper-parameters such as `learning_rate`\n",
        "- `hp.quniform('name', min, max, step)`: sets a hyper-parameter to a value between `min` and `max` with a step size of `step`. This is useful for integer parameters like `max_depth`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4GmTBFp9i3c"
      },
      "outputs": [],
      "source": [
        "#collapse-show\n",
        "\n",
        "# training params\n",
        "lgb_reg_params = {    \n",
        "    'boosting_type':    hp.choice('boosting_type', ['gbdt', 'goss']),    \n",
        "    'objective':        'rmse',\n",
        "    'metrics':          'rmse',\n",
        "    'n_estimators':     10000,\n",
        "    'learning_rate':    hp.uniform('learning_rate',  0.0001, 0.3),\n",
        "    'max_depth':        hp.quniform('max_depth',          1,  16, 1),\n",
        "    'num_leaves':       hp.quniform('num_leaves',        10,  64, 1),\n",
        "    'bagging_fraction': hp.uniform('bagging_fraction',  0.3,   1),\n",
        "    'feature_fraction': hp.uniform('feature_fraction',  0.3,   1),\n",
        "    'lambda_l1':        hp.uniform('lambda_l1',           0,   1),\n",
        "    'lambda_l2':        hp.uniform('lambda_l2',           0,   1),\n",
        "    'silent':           True,\n",
        "    'verbosity':        -1,\n",
        "    'nthread' :         4,\n",
        "    'random_state':     77,\n",
        "}\n",
        "\n",
        "# evaluation params\n",
        "lgb_fit_params = {\n",
        "    'eval_metric':           'rmse',\n",
        "    'early_stopping_rounds': 100,\n",
        "    'verbose':               False,\n",
        "}\n",
        "\n",
        "# combine params\n",
        "lgb_space = dict()\n",
        "lgb_space['reg_params'] = lgb_reg_params\n",
        "lgb_space['fit_params'] = lgb_fit_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrsZ2Hbx9i3c"
      },
      "source": [
        "Next, we create `HPOpt` object that performs tuning. We can avoid this in a simple tuning task, but defining an object gives us more control of the optimization process, which is useful with a custom loss. We define three object methods: \n",
        "- `process`: runs optimization. By default, `HPO` uses `fmin()` to minimize the specified loss\n",
        "- `lgb_reg`: initializes LightGBM model\n",
        "- `train_reg`: trains LightGBM and computes the loss. Since we aim to maximize profit, we simply define loss as negative profit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c45nt_Km9i3c"
      },
      "outputs": [],
      "source": [
        "# collapse-show\n",
        "class HPOpt(object):\n",
        "\n",
        "    # INIT\n",
        "    def __init__(self, x_train, x_test, y_train, y_test):\n",
        "        self.x_train = x_train\n",
        "        self.x_test  = x_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test  = y_test\n",
        "\n",
        "    # optimization process\n",
        "    def process(self, fn_name, space, trials, algo, max_evals):\n",
        "        fn = getattr(self, fn_name)\n",
        "        try:\n",
        "            result = fmin(fn        = fn, \n",
        "                          space     = space, \n",
        "                          algo      = algo, \n",
        "                          max_evals = max_evals, \n",
        "                          trials    = trials)\n",
        "        except Exception as e:\n",
        "            return {'status': STATUS_FAIL, 'exception': str(e)}\n",
        "        return result, trials\n",
        "    \n",
        "    \n",
        "    # LGBM initialization\n",
        "    def lgb_reg(self, para):\n",
        "        para['reg_params']['max_depth']  = int(para['reg_params']['max_depth'])\n",
        "        para['reg_params']['num_leaves'] = int(para['reg_params']['num_leaves'])\n",
        "        reg = lgb.LGBMRegressor(**para['reg_params'])\n",
        "        return self.train_reg(reg, para)\n",
        "\n",
        "    \n",
        "    # training and inference\n",
        "    def train_reg(self, reg, para):\n",
        "        \n",
        "        # fit LGBM\n",
        "        reg.fit(self.x_train, self.y_train,\n",
        "                eval_set              = [(self.x_train, self.y_train), (self.x_test, self.y_test)], \n",
        "                sample_weight         = self.x_train['simulationPrice'].values,\n",
        "                eval_sample_weight    = [self.x_train['simulationPrice'].values, self.x_test['simulationPrice'].values],\n",
        "                **para['fit_params'])\n",
        "        \n",
        "        # inference\n",
        "        if target_transform:      \n",
        "            preds = postprocess_preds(reg.predict(self.x_test)**2)\n",
        "            reals = self.y_test**2\n",
        "        else:\n",
        "            preds = postprocess_preds(reg.predict(self.x_test))\n",
        "            reals = self.y_test\n",
        "\n",
        "        # compute loss [negative profit]\n",
        "        loss = np.round(-profit(reals, preds, price = self.x_test['simulationPrice'].values))\n",
        "                      \n",
        "        return {'loss': loss, 'status': STATUS_OK}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKgFcJUk9i3c"
      },
      "source": [
        "To prevent overfitting, we perform tuning on a different subset of data compared to the models trained in the previous section by going one day further in the past."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLGi4Efs9i3c",
        "outputId": "62c35cb2-692a-4e86-e5ab-ca7c6e4bbcfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------\n",
            "- train period days: 28 -- 143 (n = 1213708)\n",
            "- valid period days: 158 -- 158 (n = 10463)\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# collapse-hide\n",
        "\n",
        "# validation dates\n",
        "v_end   = 158          # 1 day before last validation fold in code_03_modeling\n",
        "v_start = v_end        # same as v_start\n",
        "\n",
        "# training dates\n",
        "t_start = 28           # first day in the data\n",
        "t_end   = v_start - 15 # validation day - two weeks\n",
        "\n",
        "# extract index\n",
        "train_idx = list(X[(X.day_of_year >= t_start) & (X.day_of_year <= t_end)].index)\n",
        "valid_idx = list(X[(X.day_of_year >= v_start) & (X.day_of_year <= v_end)].index)   \n",
        "\n",
        "# extract samples\n",
        "X_train, y_train = X.iloc[train_idx][features], y.iloc[train_idx]\n",
        "X_valid, y_valid = X.iloc[valid_idx][features], y.iloc[valid_idx]\n",
        "    \n",
        "# target transformation\n",
        "if target_transform:\n",
        "    y_train = np.sqrt(y_train)\n",
        "    y_valid = np.sqrt(y_valid)\n",
        "\n",
        "# information\n",
        "print('-' * 65)\n",
        "print('- train period days: {} -- {} (n = {})'.format(t_start, t_end, len(train_idx)))\n",
        "print('- valid period days: {} -- {} (n = {})'.format(v_start, v_end, len(valid_idx)))\n",
        "print('-' * 65)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KlxiDgx9i3c"
      },
      "source": [
        "Now, we just need to instantiate the `HPOpt` object and launch the tuning trials! The optimization will run automatically, and we would only need to extract the optimized values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6jT3MxR9i3c",
        "outputId": "63dd9f7b-2cad-4396-d00d-5d0f965b0129"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best meta-parameters:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'boosting_type': 'goss',\n",
              " 'objective': 'rmse',\n",
              " 'metrics': 'rmse',\n",
              " 'n_estimators': 10000,\n",
              " 'learning_rate': 0.004012,\n",
              " 'max_depth': 10,\n",
              " 'num_leaves': 64,\n",
              " 'bagging_fraction': 0.934688,\n",
              " 'feature_fraction': 0.668076,\n",
              " 'lambda_l1': 0.280133,\n",
              " 'lambda_l2': 0.589682,\n",
              " 'silent': True,\n",
              " 'verbosity': -1,\n",
              " 'nthread': 4,\n",
              " 'random_state': 77}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# collapse-show\n",
        "\n",
        "# instantiate objects\n",
        "hpo_obj = HPOpt(X_train, X_valid, y_train, y_valid)\n",
        "trials  = Trials() \n",
        "\n",
        "# perform tuning\n",
        "lgb_opt_params = hpo_obj.process(fn_name   = 'lgb_reg',\n",
        "                                 space     = lgb_space, \n",
        "                                 trials    = trials, \n",
        "                                 algo      = tpe.suggest, \n",
        "                                 max_evals = tuning_trials)  \n",
        "\n",
        "# merge best params to fixed params\n",
        "params = list(lgb_opt_params[0].keys())\n",
        "for par_id in range(len(params)):\n",
        "    lgb_reg_params[params[par_id]] = lgb_opt_params[0][params[par_id]]\n",
        "    \n",
        "# postprocess\n",
        "lgb_reg_params['boosting_type'] = boost_types[lgb_reg_params['boosting_type']]\n",
        "lgb_reg_params['max_depth']     = int(lgb_reg_params['max_depth'])\n",
        "lgb_reg_params['num_leaves']    = int(lgb_reg_params['num_leaves'])\n",
        "\n",
        "# print best params\n",
        "print('Best meta-parameters:')\n",
        "lgb_reg_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjoLEQB_9i3d"
      },
      "source": [
        "Done! Now we can save the optimized values and import them when setting up the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCpGmNS19i3d"
      },
      "outputs": [],
      "source": [
        "# collapse-hide\n",
        "par_file = open('../lgb_meta_params.pkl', 'wb')\n",
        "pickle.dump(lgb_reg_params, par_file)\n",
        "par_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "___u0GFa9i3d"
      },
      "source": [
        "# 6. Closing words\n",
        "\n",
        "This blog post has finally come to an end. Thank you for reading! \n",
        "\n",
        "We looked at important stages of our solution and covered steps such as data aggregation, feature engineering, custom loss functions, target transformation and hyper-parameter tuning.\n",
        "\n",
        "Our final solution was a simple ensemble of multiple LightGBM models with different features and training options discussed in this post. If you are interested in the ensembling part, you can find the codes in my [Github](https://github.com/kozodoi/DMC_2020).\n",
        "\n",
        "Please feel free to use the comment window below to ask questions and stay tuned for the next editions of [Data Mining Cup](https://www.data-mining-cup.com)!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}